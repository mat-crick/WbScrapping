{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from dateutil.parser import parse\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def __login(username_ig, password_ig):\n",
    "    username = WebDriverWait(driver, 10).\\\n",
    "        until(EC.element_to_be_clickable\\\n",
    "            ((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "\n",
    "    password = WebDriverWait(driver, 10).\\\n",
    "        until(EC.element_to_be_clickable\\\n",
    "            ((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "    #Cleaning the fields\n",
    "    username.clear()\n",
    "    username.send_keys(username_ig)\n",
    "    password.clear()\n",
    "    password.send_keys(password_ig)\n",
    "\n",
    "    #Login\n",
    "    login_button = WebDriverWait(driver, 10)\\\n",
    "        .until(EC.element_to_be_clickable\\\n",
    "            ((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "    #Skipping not now\n",
    "    not_now = WebDriverWait(driver, 10).\\\n",
    "        until(EC.element_to_be_clickable\\\n",
    "            ((By.XPATH, '//button[contains(text(), \"Not Now\")]'))).click()\n",
    "    # not_now = WebDriverWait(driver, 10).\\\n",
    "    #     until(EC.element_to_be_clickable\\\n",
    "    #         ((By.XPATH, '//button[contains(text(), \"Not Now\")]'))).click()\n",
    "\n",
    "\n",
    "def search( keyword):\n",
    "    searchButton = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@href='#']\"))).click()\n",
    "    searchbox = WebDriverWait(driver, 10)\\\n",
    "        .until(EC.element_to_be_clickable\\\n",
    "            ((By.XPATH, \"//input[@placeholder='Search']\")))\n",
    "    searchbox.clear()\n",
    "    searchbox.send_keys('#' + keyword)\n",
    "\n",
    "\n",
    "    # Wait for 5 seconds\n",
    "    time.sleep(5)\n",
    "    searchbox.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    searchbox.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "def __get_links(nscrolls, scroll_pause_time):\n",
    "    '''\n",
    "    Getting posts links\n",
    "    '''\n",
    "    saved_links = {}\n",
    "    rank = 0\n",
    "    # Get scroll height\n",
    "    last_height = \\\n",
    "        driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for j in tqdm(range(nscrolls)):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        links = driver.find_elements(by=By.TAG_NAME, value='a') \n",
    "        # print(\"All links:\")\n",
    "        # print(links)\n",
    "        valid_links =  filter_links(links)\n",
    "\n",
    "        for i in range(len(valid_links)):\n",
    "            link = valid_links[i].get_attribute('href')\n",
    "            # print(link)\n",
    "            if link not in saved_links.keys():\n",
    "                saved_links[link] = rank\n",
    "                rank += 1\n",
    "                \n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = \\\n",
    "            driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            # If heights are the same it will exit the function\n",
    "            break\n",
    "        last_height = new_height\n",
    "    with open(\"./interim_data/links.txt\",'w') as fo:\n",
    "            fo.write(str(saved_links))\n",
    "    return saved_links\n",
    "                \n",
    "\n",
    "def filter_links(links):\n",
    "    '''\n",
    "    Filter post links\n",
    "    '''\n",
    "    post_links = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            if '.com/p/' in link.get_attribute('href'):\n",
    "                post_links.append(link)\n",
    "        except:\n",
    "            continue\n",
    "    return post_links\n",
    "\n",
    "\n",
    "def filter_links_new(links):\n",
    "    '''\n",
    "    Filter post links\n",
    "    '''\n",
    "    post_links = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            if '.com/p/' in link:\n",
    "                post_links.append(link)\n",
    "        except:\n",
    "            continue\n",
    "    return post_links\n",
    "\n",
    "def get_links_from_file(filename):\n",
    "    g1 = pd.read_csv(f'./GSERP_links/{filename}.csv')\n",
    "    g1['url'] = g1['url'].apply(lambda x: x.split(\"/?\")[0])\n",
    "    valid_post_links = filter_links_new(g1['url'].to_list())\n",
    "    ranks = list(range(len(valid_post_links)))\n",
    "    return dict(zip(valid_post_links, ranks))\n",
    "\n",
    "def get_data_v(nscrolls, scroll_pause_time, filename=None) -> dict:\n",
    "    '''\n",
    "    Get all hashtag data\n",
    "    '''\n",
    "    # links = get_links_from_file(filename)\n",
    "    # logger.info(str(len(links)) + \" links were found.\")\n",
    "    links = __get_links(nscrolls, scroll_pause_time)\n",
    "    processed_data = []\n",
    "    for link,rank in tqdm(links.items()):\n",
    "        infos = {}\n",
    "    #Accessing the post\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            date_elem = driver.find_elements(By.XPATH,\"//time\")\n",
    "            post_date = date_elem[0].get_attribute(\"datetime\")    # \n",
    "            post_date = parse(post_date)\n",
    "        except:\n",
    "            continue\n",
    "        # Check specific date range\n",
    "        # if (post_date > datetime.datetime(2023,4,30,tzinfo=pytz.UTC) ) or (post_date< datetime.datetime(2022,9,1,tzinfo=pytz.UTC)):\n",
    "        #     continue\n",
    "        try:\n",
    "            likes_elem = driver.find_element(By.XPATH,\"//meta[@property='og:description']\")\n",
    "            likes_elem_cont = likes_elem.get_attribute(\"content\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            likes, remaining = likes_elem_cont.split(\" likes, \")\n",
    "        except:\n",
    "            likes = \"\"\n",
    "            remaining = likes_elem_cont\n",
    "        try:\n",
    "            comments, remaining = remaining.split(\" comments\")\n",
    "        except:\n",
    "            comments = \"\"\n",
    "        try:\n",
    "            _, remaining = remaining.split(\" (@\")\n",
    "            user_h, remaining = remaining.split(\") on Instagram: \")\n",
    "        except:\n",
    "            user_h = \"\"\n",
    "            \n",
    "        remaining = remaining.replace('\\n','')\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            desc_xpath = '/html/body/div[2]/div/div/div[2]/div/div/div/div[1]/div[1]/div[2]/section/main/div/div[1]/div/div[2]/div/div[2]/div/div/ul/div/li/div/div/div[2]/div[1]/h1'\n",
    "            desc_elem = driver.find_element(By.XPATH,desc_xpath)\n",
    "            cont = desc_elem.get_attribute('innerHTML')\n",
    "            caption = BeautifulSoup(cont).get_text()\n",
    "        except:\n",
    "            caption = remaining\n",
    "        # print(caption)\n",
    "        image_elements = driver.find_elements(by=By.TAG_NAME, value='img')\n",
    "        # images = driver.find_elements_by_tag_name('img')\n",
    "        image_links = [image.get_attribute('src') for image in image_elements]\n",
    "        image_links = image_links[2:] #IG logo and Profile picture\n",
    "\n",
    "        infos[\"Publication_date\"] = post_date\n",
    "        infos[\"#likes\"] = likes\n",
    "        infos[\"#comments\"] = comments\n",
    "        infos[\"rank\"] = rank\n",
    "        infos[\"Caption\"] = caption\n",
    "        infos[\"PostLink\"] = link\n",
    "        infos[\"PostID\"] = link.split(\"/p/\")[1].rstrip(\"/\")\n",
    "        infos[\"ImageLink\"] = link+\"media?size=l\"\n",
    "        infos[\"user_handle\"] = \"@\"+user_h\n",
    "        # print(likes_elem_cont)\n",
    "        if infos not in processed_data and infos is not None:\n",
    "            processed_data.append(infos)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://www.instagram.com\")\n",
    "__login(\"datacrick3\", \"Shamik@40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file gucci\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:56<00:00,  5.88s/it]\n",
      "100%|██████████| 309/309 [22:02<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file prada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:48<00:00,  5.61s/it]\n",
      "100%|██████████| 311/311 [22:24<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# Using links from files\n",
    "\n",
    "# \"chanel\", \"hermes\",\"dior\",\n",
    "filenames = [ \"chanel\", \"hermes\",\"dior\",\"gucci\", \"prada\" ] #['gucci_c','gucci_n','chanel_c','chanel_n','dior_c','dior_n','hermes_c','hermes_n','prada_c','prada_n']    #'gucci_c','gucci_n','chanel_c','chanel_n',\n",
    "for fname in filenames:\n",
    "    search(fname)\n",
    "    print(f\"working on file {fname}\")\n",
    "    data = get_data_v(nscrolls = int(30),\n",
    "                                scroll_pause_time = 5, filename=  fname)\n",
    "    df = pd.DataFrame(data)\n",
    "    brand_name = \"#\"+fname.split('_')[0]\n",
    "    df['Brand_name'] = brand_name\n",
    "    df = df[['Brand_name','PostID','PostLink','ImageLink','Publication_date','user_handle',\"Caption\",'#likes', '#comments']]\n",
    "    df.to_csv(f\"./outputs_recent/{fname}_output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
